<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="zhenyuen.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="zhenyuen.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-09-15T14:06:11+00:00</updated><id>zhenyuen.github.io/feed.xml</id><title type="html">Zhen Yuen</title><subtitle>My personal blog and portfolio. </subtitle><entry><title type="html">my reflections as a software engineer (intern)</title><link href="zhenyuen.github.io/blog/2023/my-reflections/" rel="alternate" type="text/html" title="my reflections as a software engineer (intern)"/><published>2023-09-06T00:00:00+00:00</published><updated>2023-09-06T00:00:00+00:00</updated><id>zhenyuen.github.io/blog/2023/my-reflections</id><content type="html" xml:base="zhenyuen.github.io/blog/2023/my-reflections/"><![CDATA[<h2 id="about">About</h2> <p>As my internship at Visa draws to a close, I have some reflections on myself as a software engineer (SWE). This post isn’t about providing advice but rather sharing my thoughts on what it means to be a good SWE (in my opinion), and how I’m working towards that goal. If there are any experienced readers out there, please feel free to get in touch. I would love to hear about your thoughts :)</p> <h2 id="my-background">My background</h2> <p>During my time at ARM, I had the privilege of working with the build team, under the guidance of Matt, a principal software engineer. It’s hard to put into words how much I’ve learned from him and the team in just a few months – and it was what inspired me to become not just a good software engineer, but a great one. Working with the build team exposed me to the realities of software engineering. In school, we’re taught how to write the ‘perfect’ code along with proper documentation. However, in the industry, when ever-changing business requirements and tight deadlines come into play, good coding practices can start to vanish, leaving what’s known as “spaghetti code”. If it works, it works. The build teams’ responsibility includes fixing or refactoring codebases before they are built or compiled to ensure proper sharding and caching for remote build execution. There, I’ve gotten a slight taste of what constitutes good or bad code.</p> <p>I believe a good software engineer can balance the trade-off between writing quick and dirty code versus striving for perfection. This requires the ability to recognize patterns in code structure and apply the appropriate design patterns, keeping things decoupled and modular while minimizing abstractions. Here, I won’t focus on this aspect but rather on another -— writing clean and elegant code with the help of data structures and algorithms (DSA) knowledge (here, I’ll be talking about graph problems specifically).</p> <p>Having a strong understanding of data structures and algorithms forms the foundation of a good software engineer. Unfortunately, many people only study DSA to pass SWE interviews and never apply it in their work. My internship at Visa as a backend engineer, a role that comprises a large majority of SWE positions, shed light on why this happens. We often rely on well-written libraries, and with looming deadlines, it’s hard to justify writing proprietary modules when suitable and sufficiently good alternatives are available. It was only when I attempted a timed online assessment in C++ and watched <a href="https://www.youtube.com/watch?v=V8DGdPkBBxg&amp;t=1644s&amp;pp=ygULamFuZSBzdHJlZXQ%3D">Jane Street’s mock SWE interview with Grace and Nolan</a> on YouTube that I realized I had fallen into this trap too.</p> <p>Before delving further into the assessment, if you’d like to see how DSA helps in writing clean and elegant code, try <a href="https://leetcode.com/problems/valid-number/">this Leetcode problem</a>. My initial solution, like many, contained numerous if-else statements to handle edge cases. However, a much more elegant solution treats the problem as a finite state machine, resulting in a significantly cleaner solution. While developers frequently use Regex expressions in our work, most people fail to appreciate the beauty, elegance, and efficiency behind their implementation, which also leverages the same underlying idea (feel free to look up finite automata :)).</p> <h2 id="the-assessment">The assessment</h2> <p>The assessment involved taking a list of three integers, corresponding to <code class="language-plaintext highlighter-rouge">job_id</code>, <code class="language-plaintext highlighter-rouge">job_duration</code>, and <code class="language-plaintext highlighter-rouge">next_job_id</code>, and printing the list of job chains with their details to stdout (for those who are curious, the overall goal is to format the outputs from a concurrenct logging tool). Each job chain includes details like <code class="language-plaintext highlighter-rouge">starting_id</code>, <code class="language-plaintext highlighter-rouge">ending_id</code>, <code class="language-plaintext highlighter-rouge">total_duration</code>, and <code class="language-plaintext highlighter-rouge">average_duration</code>. A job chain is essentially a sequence of jobs that terminates when <code class="language-plaintext highlighter-rouge">next_job_id=0</code>. I was provided with one simple example: <code class="language-plaintext highlighter-rouge">[[1, 2, 3], [2, 5, 0], [3, 4, 0]]</code>. In this case, we have two job chains: <code class="language-plaintext highlighter-rouge">[[1, 2, 3], [3, 4, 0]]</code> and <code class="language-plaintext highlighter-rouge">[[2, 5, 0]]</code>. In case of bad inputs, the program should print <code class="language-plaintext highlighter-rouge">Malformed inputs</code>.</p> <p>My initial approach was to parse the input and apply DFS, starting from job id <code class="language-plaintext highlighter-rouge">1</code> (for this example), traversing related jobs and summing up the durations (alternatively, you can view it as a linked list). Immediately, I begun coding right away. However, as I progressed, I began to consider malformed inputs. Bad characters? I shoud be able to handle that in the parsing stage with – you guessed right – if else statements.</p> <p>Then it hit me. What happens if there is a loop in the specified job ids? What happens if the starting job comes AFTER the intermediate or final job in the input list (i.e., <code class="language-plaintext highlighter-rouge">[2, 5, 0], [3, 4, 0]], [[1, 2, 3]</code>). There were no assumptions given, no additional examples, and the possibilities were endless. As the concerns piled on, I had to modify my code to account for these scenarios as it slowly devolved into a huge mess. I could no longer print out the job chains as required as I traversed the data – if an error occured halfway, I should not have printed out the valid job chains (if any) in the first place. Hence, I had to move everything into a <code class="language-plaintext highlighter-rouge">std::stringstream</code> buffer instead (and flush it in the end if no error occurs), which also increased the overally memory complexity and slowed things down as string processing are slow in general. Unsurprisingly, I failed the assessment.</p> <p>Right after it ended, I immediately realized I could have used a directed acyclic graph and traversed the nodes in topological order. When I received the question under time pressure, my first instinct as an inexperienced SWE was to use a quick and dirty solution, then slowly refining the code as what is usually done in an agile setup. I started typing my new solution which was much cleaner. It was then I developed my own approach to building software. In my (limited) experience, most applications can be brokened down into two parts:</p> <ol> <li> <p>The processing part that handles inputs, processes them, and produces outputs. This is where our knowledge of DSA (Data Structures and Algorithms) can be often applied.</p> </li> <li> <p>The framework which serves as the glue that ties everything together, including all the processing. While DSA can be applied at times, usually it’s the use of appropriate design patterns and abstractions that benefit the most.</p> </li> </ol> <p>Focusing on (1), I adopted a 4-stage process for most of my recent works:</p> <ol> <li>Parse the inputs, and identify what constitutes a graph node and their relationships. In most processing workloads, more often than not we will find that the given problem is reducible into a graph problem, you just have to look hard enough :)</li> <li>Define the graph or components.</li> <li>For the desired operation, build the required computational graph/sequence – traverse the nodes and perform the necessary checks (e.g., cycle detection). Rather than constructing the desired output as we traverse the graph, leave that to later, and just output the correct traversal sequence (if valid). This is an important point, as I will explain later.</li> <li>Execute the computation using the pre-computed traversal sequence.</li> </ol> <p>These 4 steps are applicable to most problems, albeit doing some necessary adjustments depending on the problem. Applying this metholodgy to my assessment:</p> <ol> <li>Each node and edges can be uniquely identified by the <code class="language-plaintext highlighter-rouge">job_id</code> and the corresponding <code class="language-plaintext highlighter-rouge">next_job_id</code>.</li> <li>When defining the graph, we leave out <code class="language-plaintext highlighter-rouge">job_durations</code> for now, as they do not matter when building our computational graph (note that the duration are NOT edge weights).</li> <li>Traverse the graph in a topological order (DFS) and perform cycle detection at the same time. In addition, cache the traversal sequence.</li> <li>Once we have the correct and valid sequence, just use a basic for-loop and perform the required operations – summing up the durations, computing averages, etc.</li> </ol> <p>A sample code is provided in my <a href="https://github.com/zhenyuen/assessments/tree/main">Git repository</a></p> <p>You can certainly combine steps 3 and 4 together – traverse and process at the same time – that is what most Leetcode problems do, as they are just toy examples. In practice, however, significant processing and aggregations are often necessary, resulting in code that may appear messy, especially when recursion is involved. Storing all intermediate processing on the stack is not ideal, and ideally, no processing should occur if the build is bound to fail initially. This is why I prefer separating both steps. In addition, try keeping the graphs in their most minimal representations – we only need sufficient information to build and verify the computation graph, nothing more.</p> <h2 id="the-video">The video</h2> <p>So, how does <a href="https://www.youtube.com/watch?v=V8DGdPkBBxg&amp;t=1644s&amp;pp=ygULamFuZSBzdHJlZXQ%3D">the video</a> relate to my example?</p> <p>Here, Nolan was tasked with implementing a unit conversion tool during his interview. For instance, how do you convert 1 cm into inches? It seems straightforward, doesn’t it? When I first watched the video a few months ago, my initial thought was to use maps and if-else statements. Why not? 1 inch equals 2.54 cm, so we could create a map specifying the multiplication/division factor. When Nolan first suggested approaching it as a graph problem and using BFS on his first attempt, I questioned if it was really necessary.</p> <p>Looking back at the video now, I have a different perspective. When you delve deeper into the problem, you’ll appreciate the elegance of his approach. As we expand the number of available units for conversion or even define custom units or metrics, his approach allows us to extend his code naturally (adding more nodes/edges). This is preferable to maintaining a map with an exponentially growing number of entries and a multitude of if-else statements.</p> <p>Some of my points about separating the construction of the computation graph from its execution can be observed in the video. At one point, Nolan encountered minor challenges when incorporating processing into BFS, such as determining what value to return. Additionally, he was using Python, whereas other languages like C++ may not appear as elegant when returning tuples with many elements of varying types.</p> <p>Under the same <a href="https://github.com/zhenyuen/assessments/tree/main/si_imperial_converter">repository</a>, I’ve added my own take on the problem – although I have a much smaller set of conversion units but the same underying logic. You may notice a CMakeLists.txt file that is incomplete, with numerous debugging statements controlled by a global debugging flag. I’m still in the process of learning how to write proper test benches in C++ and utilize CMake effectively without relying on my preferred Makefiles. Therefore, I apologize for any inconvenience this may cause.</p> <h2 id="wrap-up">Wrap-up</h2> <p>A code is truly bug-free only when it can be formally proven with mathematics.</p> <p>Although this doesn’t often apply in practical scenarios, it doesn’t mean we shouldn’t strive for it. By breaking down our code into smaller components and employing well-proven algorithms to tackle specific tasks and processes, we inch-closer to producing higher-quality, error-free code.</p> <p>During my time at Visa, one of my favorite projects involved creating a tool that generated configuration files dynamically at runtime. Through the use of Regex expressions, lookup tables, and a straightforward depth-first search (DFS) approach, I successfully transformed what initially appeared to be an $O(n^3)$ problem into an $O(n^2)$ solution with some understanding of DSA (Data Structures and Algorithms).</p> <p>I hope this write-up wasn’t too lengthy, and I hope it sheds light on my thought process and journey towards becoming a better software engineer. Thank you for reading through to the end!</p>]]></content><author><name>Zhen Yuen Chong</name></author><summary type="html"><![CDATA[a blog post detailing my thoughts on what it means to be a good software engineer, and how I am learning to become one.]]></summary></entry><entry><title type="html">winning the cambridge university project prize award in 80+ hours (part 1).</title><link href="zhenyuen.github.io/blog/2023/riscv-processor/" rel="alternate" type="text/html" title="winning the cambridge university project prize award in 80+ hours (part 1)."/><published>2023-07-16T00:00:00+00:00</published><updated>2023-07-16T00:00:00+00:00</updated><id>zhenyuen.github.io/blog/2023/riscv-processor</id><content type="html" xml:base="zhenyuen.github.io/blog/2023/riscv-processor/"><![CDATA[<h2 id="about">About</h2> <p>Every undergraduate under the Cambridge Engineering Tripos IIA (3rd Year) must undertake two projects towards the end of the academic year. My choices were the RISC-V Processor (this) and Machine Learning, with a recommended time-frame of 80 hours per project spread across 4 weeks. Yes, I apologize for the mildly misleading title for those who thought this was a hackathon 😄.</p> <p>This project involved improving an unoptimized RISC-V processor running on an iCE40 FPGA in a tiny wafer-scale 2.15x2.50 mm WLCSP package, using a completely open-source toolchain (Yosys, Project IceStorm, NextPNR, etc). The baseline design provided is the Sail RISC-V processor to be optimized. The Sail RISC-V processor uses the RV32I instruction set architecture (ISA) and features a 5-stage pipeline. The iCE40 UltraPlus Mobile Development Platform Board consists of four onboard iCE4UP5K FPGAs, labeled A to D. However, only FPGA D will be considered in this project. All modifications were done in Verilog, and the final processor design was synthesized using Yosys. The project culminated in a competition between teams to achieve designs on the Pareto frontier for some selected competition binaries.</p> <p><a href="https://github.com/zhenyuen/sail-riscv">GitHub link</a></p> <h2 id="my-background">My background</h2> <p>With an introductory understanding of computer organization and zero practical experience in Verilog, FPGAs, and open-source toolchains in the embedded space, I knew this project was going to be challenging right from the start. Furthermore, the information provided by our instructors and handouts was minimal at best, so I was mostly on my own. Given the limited time frame, compromises had to be made when implementing my proposed modifications while following the traditional FPGA design process. Hence, this blog is not meant to be educational but rather to shed some light on my thought process. If something feels too simple or “hacky,” you are most likely right 😄.</p> <p>Here is a list of the main resources or materials that helped me through this project:</p> <ul> <li><a href="https://www.amazon.co.uk/Computer-Organization-Design-RISC-V-Architecture/dp/0128122757">Computer Organization and Design RISC-V edition: The hardware software interface</a></li> <li><a href="https://riscv.org/wp-content/uploads/2017/05/riscv-spec-v2.2.pdf">The RISC-V Instruction Set Manual</a></li> <li><a href="https://f-of-e.org">Foundations of Embedded Systems</a></li> </ul> <p>In addition, I would recommend my readers to learn how to read schematic diagrams. Most users (like me) have only used beginner-friendly embedded platforms such as Arduino or Raspberry Pi that come equipped with easy-to-read documentations and ‘pretty’ pin layouts. Credits to my friend, Ye Heng (do check out his <a href="https://zen.infinus-electronics.net/projects/">blog</a>) for teaching me how to read the Lattice iCE40 FPGA pin-out diagram. The pins can be used to connect an oscilloscope to the FPGA, and the measured pin voltages can be used as flags during debugging.</p> <h2 id="tools-setup">Tools setup</h2> <p>In the initial stages of this project, the required tools that needed to be set up were Project IceStorm, Yosys, NextPNR, GTKWave, and the Diligent WaveForms Tool. The build compilation process mainly follows these steps:</p> <ol> <li>Using Yosys to synthesize the underlying hardware for the processor described in the Verilog.</li> <li>Using NextPNR for placing and routing the components involved.</li> <li>Using iceprog from Project IceStorm to configure the FPGA with the generated bit-stream.</li> </ol> <p>Both Yosys and NextPNR can provide useful metrics on the simulated design, such as the total path delay, clock frequency, and device utilization.</p> <p>Instead of having to build the toolchains locally, a <a href="https://github.com/f-of-e/gb3-resources">Docker image</a> was provided, greatly simplifying the workflow by being pre-equipped with the aforementioned tools. Additionally, the use of a Docker container can avoid dependency conflicts by ensuring the desired run-time environment is used.</p> <p>The Docker container also comes equipped with <a href="https://github.com/physical-computation/sunflower-embedded-system-emulator">Sunflower</a>, which is a full-system emulator. It can take compiled binaries and emulate them instruction by instruction for multiple complete embedded systems networked over wired or wireless connections, or integrated into a single chip and communicating over shared memory. Although the processor design used by Sunflower has no connection to the RISC-V processor I would be working on, it provides useful commands, such as obtaining the fetch instruction count for some compiled binary with the specified target architecture. Note that certain commands, such as <code class="language-plaintext highlighter-rouge">dumpdistr</code>, were avoided as they cause Sunflower to crash due to an unidentified bug.</p> <h2 id="preliminary-work-and-exploration">Preliminary work and exploration</h2> <p>The simplest metrics to evaluate a processor’s performance would be the program execution time and the average cycle per instruction (CPI). \(\begin{equation} \textit{Execution Time} = \frac{N_{instructions}}{N_{programs}} \cdot \frac{N_{cycles}}{N_{instructions}} \cdot \textit{Clock Rate}\label{eq:1__1} \end{equation}\)</p> <p>There are numerous ways to calculate the average CPI. The RV32I base integer instruction set architecture (ISA) consists of 47 unique in- structions and 31 general-purpose registers (x1-x31). The number of clock cycles per instruction can be obtained by using the RDCYCLE instruction, which reads the lower 32 bits of the cycle control status (CSR) register, holding the clock cycle count executed by the processor core. Alternatively, a comprehensive test bench can be written in Verilog, involving multiple modules such as the ALU and memory unit, working together to complete a full instruction pass through the pipeline (IF, ID, EX, MEM, WB). By simulating this test bench, the corresponding number of clock cycles can be obtained. However, due to time constraints, writing such a test bench is complex and infeasible.</p> <p>Instead, to approximate the average CPI, a simpler method was employed. The execution time of a program is measured, and then divided by the dynamic instruction count. It is important to note that this approach has potential pitfalls, as the program contains a variety of different instructions with a non-uniform distribution in terms of their frequency and counts, which can distort the average CPI. Additionally, using different compiler versions or run- time environments may result in a different instruction mix. For more sophisticated benchmarks, interested readers can source them from other sources such as <a href="http://fpbench.org">FPBench</a>. Armed with the power of Sunflower and the rough assumption that the compiled binary for Sunflower and the RISC-V processor would be similar, the average CPI of my design could be measured. One may question the validity of my assumptions. After writing a variety of benchmarks in C and inspecting the corresponding assembly files generated via <code class="language-plaintext highlighter-rouge">objdump</code>, the static instruction counts for both targets are relatively similar.</p> <p>An interesting observation is that the average CPI computed for benchmark B2 is 0.820, which is lower than 1. This should not be possible for the baseline design, which is a single-core pipeline processor without super-scalar execution. Further investigation reveals that the static instruction count generated by the compiler for the Sunflower emulator is 25751, nearly twice the count for the FPGA, which is 12806 (this can be measured using the <code class="language-plaintext highlighter-rouge">wc</code> command by passing the assembly file generated by <code class="language-plaintext highlighter-rouge">objdump</code>). This disparity arises due to the use of multiplication operations in B2, which the hardware lacks support for. Consequently, the compiler performs software-level multiplication, which may differ depending on the flags and platform used.</p> <p>In the baseline design, the instruction module’s implementation was left to the Yosys synthesizer, utilizing only the available logic cells (LC). However, if the target program exceeds the capacity of the LCs, the program fails to execute. Currently, the limit is set to 1024 instruction words, as indicated by the <code class="language-plaintext highlighter-rouge">program.hex</code> file. Each word corresponds to 32 bits or 1 byte. Nevertheless, the FPGA is equipped with an additional 30 EBR (Embedded Block RAM) blocks, each having a size of 4 Kbit, resulting in a total of 120 Kbit of memory. From my understanding, only the data memory is currently stored in the EBR. Consequently, this design choice of storing the instruction memory in the LCs can lead to varying critical path delays, depending on the program loaded onto the FPGA. Larger programs tend to consume more LCs and have longer routing paths, resulting in increased delays along the critical path. As a consequence, this lowers the upper limit of the stable clock frequency. Additionally, slight variations in the measured critical path delays between experiments may arise.</p> <h2 id="design-strategy">Design strategy</h2> <p>The processor’s performance can be enhanced by either increasing its clock frequency or decreasing the clock cycles per instruction (CPI), both of which lead to reduced execution times per program, as shown in (1). The design strategy for achieving these improvements involves the following stages: (i) Understanding and identifying limitations in the baseline design. (ii) Improving or adding features to reduce CPI. (iii) Increasing the processor’s clock frequency by reducing the critical path delay.</p> <p>The baseline processor is designed with a 5-stage pipeline, which includes the fetch (IF), decode (ID), execute (EX), memory access (MEM), and register write-back (WB) stages. Each instruction requires a minimum of 5 clock cycles to complete, resulting in a throughput of 1 CPI, where one instruction is completed per clock cycle. However, control hazards, such as those arising from conditional branches, prevent the processor from achieving a perfect CPI of 1. When a conditional branch is encountered, the condition needs to be evaluated at the EX stage to determine whether the branch should be taken or not for the next instruction. This introduces bubbles in the pipeline, where no operation (NOP) can be performed until the EX stage of the conditional branch is completed. To mitigate the impact of control hazards and reduce pipeline stalls, branch predictors are employed. Branch predictors make predictions about the outcome of conditional branches based on past branch behaviour, allowing the proces- sor to continue executing subsequent instructions based on the prediction. If the prediction is correct, the pipeline can proceed without stalls. However, if the prediction is incorrect, the processor needs to flush the incorrectly speculated instructions from the pipeline, incurring a penalty of 3 clock cycles in a 5-stage pipeline. Improving the accuracy of the branch predictor is essential to achieve a lower CPI. Accurate branch predictions help minimize pipeline stalls and the associated performance penalty, leading to improved overall performance.</p> <p>The upper stable clock frequency limit of the processor is determined by the total delay in its critical path. Certain instructions are inherently more complex and may require more time to complete compared to others in the pipeline. This complexity can be due to factors such as involving more hardware, which incurs additional latency and communication costs. When such instructions occur, the pipeline stage han- dling that specific instruction becomes the bottleneck, and the other stages of the pipeline must be stalled until the bottleneck is cleared. Consequently, the clock frequency is limited by the time required for the bottleneck stage to finish its operation. To identify the bottleneck stage and analyze the total path delay, timing tools like icetime can be utilized. These tools provide information on the path taken by signals and help identify the parts of the processor that contribute to the total path delay. By analyzing this information, it becomes possible to identify components that are causing delays and explore alternatives to mitigate their impact on the critical path. For the baseline design, the upper stable clock frequency limit is approximately 14.69 MHz.</p> <p>One approach to improve performance and increase the clock frequency is by increasing the number of pipeline stages and using smaller stages that require less time to complete. Many modern processors employ more than 5 stages in their pipeline, allowing them to achieve higher clock frequencies while maintaining a throughput of approximately 1 CPI. In our design, several modifications have been made to enhance the performance and resource usage of the processor. These modifications include adjustments to the clock frequency, improvements to the arithmetic logical unit (ALU), enhance- ments to the branch predictor, and refinements in the design of the pipeline stages. Each modification has its degree of success in improving overall performance. The main ideas behind these modifications involve removing redundant operations, optimizing the usage of available resources, and leveraging the onboard digital signal processors (DSP) present on the FPGA. By carefully considering these factors and making appropriate design choices, it is possible to achieve a Pareto optimal design that balances performance and resource usage with minimal trade-offs between the two objectives.</p> <p>To be continued …</p>]]></content><author><name>Zhen Yuen Chong</name></author><summary type="html"><![CDATA[a blog detailing my thought process behind optimizing a RISC-V processor as a beginner.]]></summary></entry><entry><title type="html">welcome</title><link href="zhenyuen.github.io/blog/2023/introduction/" rel="alternate" type="text/html" title="welcome"/><published>2023-01-17T00:00:00+00:00</published><updated>2023-01-17T00:00:00+00:00</updated><id>zhenyuen.github.io/blog/2023/introduction</id><content type="html" xml:base="zhenyuen.github.io/blog/2023/introduction/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Hi there! Not too sure what I should write for my first blog post(?) It’s 2am here, and I have just finished setting up my personal website. The goal of having this blog is to provide some insight and thoughts into my academic interests, e.g., computer systems, information theory, machine learning and quantitative finance.</p> <p>Ideally, I would love to dedicate my time to my blog and website. However, I am pretty occupied with exam preparations and job applications currently. There’s still lots of work to be done here. In time, I hope to improve the website and provide various educational materials for my fellow readers! Stay tuned :)</p> <p><strong>P.S. the next section is just me experimenting with LaTeX and the <a href="https://docs.mathjax.org/en/latest/index.html">MathJax 3 engine</a>.</strong></p> <hr/> <h2 id="equations">Equations</h2> <p>Taylors Theorem,</p> \[f(x)=\sum_{k=0}^\infty f^{(k)}(a)\frac{(x-a)^k}{k!}\] <p>Laplace Transform,</p> \[\mathcal{L}\{f(t)\}=\int_{t=0}^{\infty}f(t)e^{-st}dt\]]]></content><author><name>Zhen Yuen Chong</name></author><summary type="html"><![CDATA[a gentle introduction to my blog and website.]]></summary></entry></feed>